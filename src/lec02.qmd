# Introduction to systems of linear equations

## Introduction

The basic problem is to solve a set of $n$ **linear** equations for $n$ unknown values $x_j$, $j = 1, \ldots, n$.

**Notation**:

$$
\begin{aligned}
\text{Equation 1:} && a_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \cdots + a_{1n} x_n & = b_1 \\
\text{Equation 2:} && a_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \cdots + a_{2n} x_n & = b_2 \\
\vdots \\
\text{Equation i:} && a_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \cdots + a_{in} x_n & = b_i \\
\vdots \\
\text{Equation n:} && a_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \cdots + a_{nn} x_n & = b_n.
\end{aligned}
$$

**Notes**:

-   The values $a_{ij}$ are known as **coefficients**.

-   The **right hand side** values $b_i$ are known and are given to you as part of the problem.

-   $x_1, x_2, x_3, \ldots, x_n$ are **not** known and are what you need to find to solve the problem.

Many computational algorithms require the solution of linear equations, e.g.Â in fields such as

-   Scientific computation;
-   Network design and optimisation;
-   Graphics and visualisation;
-   Machine learning.

TODO precise examples

Typically these systems are *very* large ($n \approx 10^9$).

It is therefore important that this problem can be solved

-   accurately: we are allowed to make small errors but not big errors;
-   efficiently: we need to find the answer quickly;
-   reliably: we need to know that our algorithm will give us an answer that we are happy with.

::: {.callout-warning}
### Remark

One way to solve a system of linear equations is to compute the inverse of $A$, $A^{-1}$, directly, then the solution is found through matrix multiplication: $\vec{x} = A^{-1} \vec{b}$. This turns out to be an inefficient approach and we can do better with specialised algorithms.

```{python}
import time

import numpy as np

for size in [100, 10000]:
    print(f"\n\nTrying different approaches for problem size {size}")
    # create a system of linear equations with unique solution
    A_part = np.random.rand(size, size)
    A = np.dot(A_part, A_part.T)
    b = np.random.rand(size, 1)

    # approach 1 - invert the matrix and apply the inverse
    start_time = time.time()
    A_inv = np.linalg.inv(A)
    x = A_inv @ b
    end_time = time.time()
    time_1 = end_time - start_time
    print("Approach 1\n- inverting the matrix and applying the inverse takes ", time_1)

    # approach 2 - solve the system of linear equations
    start_time = time.time()
    x = np.linalg.solve(A, b)
    end_time = time.time()
    time_2 = end_time - start_time
    print("Approach 2\n- solving the system of linear equations takes ", time_2)

    print("Approach 2 is faster by a factor of ", time_1 / time_2)
```
:::

### General matrix-vector form

Solve the system of equations given by

$$
 \begin{pmatrix}
 a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
 a_{21} & a_{22} & a_{23} & \cdots & a_{2n} \\
 a_{31} & a_{32} & a_{33} & \cdots & a_{3n} \\
 \vdots & \vdots & \vdots & & \vdots \\
 a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn}
 \end{pmatrix}
 \begin{pmatrix}
 x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n
 \end{pmatrix} =
 \begin{pmatrix}
 b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_n
 \end{pmatrix}.
$$

In other words, given an $n \times n$ matrix $A$ and an $n$-vector $\vec{b}$, find the $n$-vector $\vec{x}$ which satisfies
$$
A \vec{x} = \vec{b}.
$$

::: {.callout-tip}
### Example 1: Temperature in a sealed room
Suppose we wish to estimate the temperature distribution inside an object:

```{python}
# | fig-cap: Image showing temperature sample points and relations in a room.
import numpy as np
from matplotlib import pyplot as plt

R = np.sqrt(0.5)
c = np.array([0.5, 0.5])
d = np.pi / 6
p = np.pi / 4

pts = (
    [np.array([0.0, 0.0]), np.array([0.5, 0.0])]
    + [c + R * np.array([np.cos(n * d + p), np.sin(n * d + p)]) for n in range(-3, 4)]
    + [np.array([0.0, 0.5])]
)


temperatures = [
    200,
    100,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    100,
]

interior_pts = [
    [0.25, 0.25],
    [0.75, 0.25],
    [0.5, 0.5],
    [1.0, 0.5],
    [0.25, 0.75],
    [0.75, 0.75],
    [0.5, 1.0],
]

edges = [
    [0, 1],
    [0, 10],
    [0, 9],
    [1, 10],
    [1, 11],
    [1, 2],
    [2, 11],
    [2, 13],
    [2, 3],
    [3, 13],
    [3, 4],
    [4, 13],
    [4, 15],
    [4, 5],
    [5, 15],
    [5, 6],
    [6, 15],
    [6, 16],
    [6, 7],
    [7, 16],
    [7, 8],
    [8, 16],
    [8, 14],
    [8, 9],
    [9, 14],
    [9, 10],
    [10, 11],
    [10, 12],
    [10, 14],
    [11, 12],
    [11, 15],
    [11, 13],
    [12, 14],
    [12, 15],
    [13, 15],
    [14, 15],
    [14, 16],
    [15, 16],
]

fig, axs = plt.subplots(1, 2)


axs[0].fill([pt[0] for pt in pts], [pt[1] for pt in pts], edgecolor="black")

for pt, t in zip(pts, temperatures):
    axs[0].plot(pt[0], pt[1], "ko")
    offset = 0.1 * (pt - c) / np.linalg.norm(pt - c)
    axs[0].text(
        pt[0] + offset[0],
        pt[1] + offset[1],
        t,
        ha="center",
        bbox={"facecolor": "white", "boxstyle": "round,pad=0.2"},
    )

for pt, t in zip(pts, temperatures):
    axs[1].plot(pt[0], pt[1], "ko")
    offset = 0.1 * (pt - c) / np.linalg.norm(pt - c)
    axs[1].text(
        pt[0] + offset[0],
        pt[1] + offset[1],
        t,
        ha="center",
        bbox={"facecolor": "white", "boxstyle": "round,pad=0.2"},
    )

for j, pt in enumerate(interior_pts):
    axs[1].plot(pt[0], pt[1], "ko")
    axs[1].text(
        pt[0] + 0.06,
        pt[1] + 0.06,
        f"$x_{{{j + 1}}}$",
        ha="center",
        bbox={"facecolor": "white", "boxstyle": "round,pad=0.2"},
    )

all_pts = pts + interior_pts

for e in edges:
    pt_A, pt_B = all_pts[e[0]], all_pts[e[1]]
    axs[1].plot([pt_A[0], pt_B[0]], [pt_A[1], pt_B[1]], "k")

for ax in axs:
    ax.set_axis_off()
    ax.set_aspect("equal")

plt.tight_layout()
plt.show()
```

We can place a network of points inside the object and use the following model: the temperature at each interior point is the average of its neighbours.

This example leads to the system:

$$
\begin{pmatrix}
  1   & -1/6 & -1/6 &  0   & -1/6 &  0   & 0 \\
 -1/6 &  1   & -1/6 & -1/6 &  0   & -1/6 & 0 \\
 -1/4 & -1/4 &  1   &  0   & -1/4 & -1/4 & 0 \\
  0   & -1/5 &  0   &  1   &  0   & -1/5 & 0 \\
 -1/6 &  0   & -1/6 &  0   &  1   & -1/6 & -1/6 \\
  0   & -1/8 & -1/8 & -1/8 & -1/8 & 1 & -1/8 \\
  0   &  0   &  0   &  0   & -1/5 & -1/5 &   1
\end{pmatrix}
\begin{pmatrix}
 x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\ x_7
\end{pmatrix} =
\begin{pmatrix}
400/6 \\ 100/6 \\ 0 \\ 0 \\ 100/6 \\ 0 \\ 0
\end{pmatrix}.
$$
:::

::: {.callout-tip}
### Example 2: Traffic network

Suppose we wish to monitor the flow of traffic in a city centre:

```{python}
# | fig-cap: Example network showing traffic flow in a city
dx = 1.0
dy = 0.7
head_proportion = 0.75


pts = np.array(
    [
        [dx, 0.0],
        [2 * dx, 0.0],
        [0.0, dy],
        [dx, dy],
        [2 * dx, dy],
        [0.0, 2 * dy],
        [dx, 2 * dy],
        [2 * dx, 2 * dy],
    ]
)

arrows = [
    # x arrows
    [np.array([2 * dx, 2 * dy]), np.array([-dx, 0.0]), "$x_1$"],
    [np.array([dx, 2 * dy]), np.array([-dx, 0.0]), "$x_2$"],
    [np.array([2 * dx, 2 * dy]), np.array([0.0, -dy]), "$x_3$"],
    [np.array([0.0, 2 * dy]), np.array([0.0, -dy]), "$x_4$"],
    [np.array([dx, dy]), np.array([-dx, 0.0]), "$x_5$"],
    [np.array([2 * dx, dy]), np.array([0.0, -dy]), "$x_6$"],
    [np.array([dx, dy]), np.array([0.0, -dy]), "$x_7$"],
    [np.array([2 * dx, 0.0]), np.array([-dx, 0.0]), "$x_8$"],
    # y arrows
    [np.array([dx, 0]), np.array([0.0, -dy]), "$y_1$"],
    [np.array([2 * dx, 0]), np.array([0.0, -dy]), "$y_2$"],
    [np.array([2 * dx + dx, 0]), np.array([-dx, 0.0]), "$y_3$"],
    [np.array([2 * dx + dx, dy]), np.array([-dx, 0.0]), "$y_4$"],
    [np.array([2 * dx + dx, 2 * dy + dy]), np.array([-dx, -dy]), "$y_5$"],
    [np.array([dx, 2 * dy + dy]), np.array([0.0, -dy]), "$y_6$"],
    [np.array([0.0, 2 * dy + dy]), np.array([0.0, -dy]), "$y_7$"],
    [np.array([0.0, 2 * dy]), np.array([-dx, 0.0]), "$y_8$"],
    [np.array([0.0, dy]), np.array([-dx, 0.0]), "$y_9$"],
    [np.array([dx, dy]), np.array([-dx, -dy]), "$y_{10}$"],
    [np.array([2 * dx, dy]), np.array([-dx, 0.0]), "$y_{11}$"],
    [np.array([dx, 2 * dy]), np.array([0.0, -dy]), "$y_{12}$"],
]

for pt in pts:
    plt.plot(pt[0], pt[1], "ko")

for arrow in arrows:
    start, direction, label = arrow
    if "x" in label:
        color = "C0"
    elif "y" in label:
        color = "C1"
    plt.arrow(
        start[0],
        start[1],
        direction[0] * head_proportion,
        direction[1] * head_proportion,
        length_includes_head=True,
        head_width=0.05,
        facecolor=color,
    )
    plt.arrow(
        start[0] + direction[0] * head_proportion,
        start[1] + direction[1] * head_proportion,
        direction[0] * (1 - head_proportion),
        direction[1] * (1 - head_proportion),
        length_includes_head=True,
        head_width=0.0,
    )
    plt.text(
        start[0] + direction[0] / 2,
        start[1] + direction[1] / 2,
        label,
        ha="center",
        va="center",
        bbox={"facecolor": "white", "boxstyle": "round,pad=0.2"},
    )

ax = plt.gca()
ax.set_axis_off()
ax.set_aspect("equal")

plt.tight_layout()
plt.show()
```

As the above example shows, it is not necessary to monitor at every single road. If we know all of the $y$ values we can calculate the $x$ values!

This example leads to the system:

$$
\begin{pmatrix}
1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
1 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & -1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & -1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & -1 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\ x_7 \\ x_8
\end{pmatrix} =
\begin{pmatrix}
y_5 \\ y_{12} - y_6 \\ y_8 - y_7 \\ y_{11} - y_4 \\
y_{11} + y_{12} - y_{10} \\ y_9 \\ y_2 - y_3 \\ y_1
\end{pmatrix}.
$$
:::

## Special types of matrices

The general matrix $A$ before the examples is known as a **full** matrix: any of its components $a_{ij}$ might be nonzero.

Almost always the problem being solved leads to a matrix with a particular structure of entries: Some entries may be known to be zero.
If this is the case then it is often possible to use this knowledge to improve the efficiency of the algorithm (in terms of both speed and/or storage).

::: {.callout-tip}
### Example 1: Triangular matrix

One common (and important) structure takes the form

$$
 \begin{pmatrix}
 a_{11} & 0 & 0 & \cdots & 0 \\
 a_{21} & a_{22} & 0 & \cdots & 0 \\
 a_{31} & a_{32} & a_{33} & \cdots & 0 \\
 \vdots & \vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn}
 \end{pmatrix}
 \begin{pmatrix}
 x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n
 \end{pmatrix} =
 \begin{pmatrix}
 b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_n
 \end{pmatrix}.
$$

-   A is a **lower triangular** matrix. Every entry above the leading diagonal is zero:

    $$
    a_{ij} = 0 \quad \text{ for } \quad j > i.
    $$

-   The *transpose* of this matrix is an **upper triangular** matrix and can be treated in a very similar manner.
:::

::: {.callout-tip}
### Example 2: Sparse matrices

**Sparse matrices** are extremely common in any application which relies on some form of *graph* structure (see both [temperature](#application-i-temperature-in-a-sealed-room) and [traffic network examples](#application-ii-traffic-network)).

-   The $a_{ij}$ typically represents some form of "communication" between vertices $i$ and $j$ of the graph, so the element is only nonzero if the vertices are connected.

-   There is no generic pattern for these entries, though there is usually one that is specific to the problem solved.

- Usually $a_{ii} \neq 0$ - the diagonal is nonzero.

-   A "large" portion of the matrix is zero.
    -   A full $n \times n$ matrix has $n^2$ nonzero entries.
    -   A sparse $n \times n$ has $\alpha n$ nonzero entries, where $\alpha \ll n$.
-   Many special techniques exist for handling sparse matrices, some of which can be used automatically within Python ([`scipy.sparse` documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html))
:::

What is the significance of these special examples?

-   In the next lecture we will discuss a general numerical algorithm for the solution of linear systems of equations.

-   This will involve **reducing** the problem to one involving a **triangular matrix** which, as we show below, is relatively easy to solve.

-   In subsequent lectures, we will see that, for *sparse* matrix systems, alternative solution techniques are available.

## Uniqueness of solutions

For the time-being we will only consider *square* systems of equations:
for which the number of equations is equal to the number of unknowns ($n$, say).

In this case the following statements are *equivalent*:

-   The linear system $A \vec{x} = \vec{b}$ has a **unique solution**.
-   There exists a matrix (let's call it $A^{-1}$) such that $A^{-1} A = I$, and we say that the matrix $A$ is **invertible**.
-   The linear system $A \vec{x} = \vec{b}$ is **non-singular**.

## Further reading

- Wikipedia: [Systems of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations) (includes a nice geometric picture of what a system of linear equations means).
- Maths is fun: [Systems of linear equations](https://www.mathsisfun.com/algebra/systems-linear-equations.html) (very basic!)
- Gregory Gundersen [Why shouldn't I invert that matrix?](http://gregorygundersen.com/blog/2020/12/09/matrix-inversion/)

