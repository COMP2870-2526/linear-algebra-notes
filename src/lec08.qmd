---
title: "Eigenvectors and eigenvalues: practical solutions"
jupyter: python3
---

```{python}
# | echo: false
import time

import numpy as np
from matplotlib import pyplot as plt
from scipy.optimize import curve_fit
from scipy.stats import special_ortho_group


```

In the previous lecture, we defined the eigenvalue problem for a matrix $A$: Finding numbers $\lambda$ (eigenvalues) and vectors $\vec{x}$ (eigenvectors) which satisfy the equation:
\begin{equation}
A \vec{x} = \lambda \vec{x}.
\end{equation}
We saw one starting point for finding eigenvalues is to find the roots of the characteristic equation: a polynomial of degree $n$ for an $n \times n$ matrix $A$. But we already have seen that this approach will be infeasible for large matrices. Instead, we will find a sequence of similar matrices to $A$ such that we can read off the eigenvalues from the final matrix.

In equations, we can say our "grand strategy" is to find a sequence of matrices $P_1, P_2, \ldots$ to form a sequence of matrices:
\begin{equation}
\label{eq:similarity_transform}
A, P_1^{-1} A P, P_2^{-1} P_1^{-1} A P_1 P_2, P_3^{-1} P_2^{-1} P_1^{-1} A P_1 P_2 P_3, \ldots
\end{equation}
If we get all the way to a diagonal matrix, at level $m$ say, then the eigenvalues are the diagonal of the matrix
\begin{equation*}
P_m^{-1} P_{m-1}^{-1} \cdots P_2^{-1} P_1^{-1} A P_1 P_2 \cdots P_{m-1} P_m,
\end{equation*}
and the eigenvectors are the columns of the matrix
\begin{equation*}
S_m = P_1 P_2 \cdots P_{m-1} P_m.
\end{equation*}
Sometimes, we only want to compute eigenvalues, and not eigenvectors, then it is sufficient to transform the matrix to be triangular (either upper or lower triangular). Then, we can read off that the eigenvalues are the diagonal entries (see @exm-eigenvalue-triangular).

## Jacobi transformations of a symmetric matrix

The Jacobi method forms a sequence of similarity transforms similar to \eqref{eq:similarity_transform}.
It is a foolproof approach for real, symmetric matrices (TODO ref section).

The key idea is to use the matrix $P_{pq}$ which is the matrix of the form
\begin{equation}
\label{eq:Pmatrix}
 P_{pq} = \begin{pmatrix}
  1 &&&&&&& \\
  & \ddots &&&&&& \\
  && c & \cdots & s &&& \\
  && \vdots & 1 & \vdots &&& \\
  && -s & \cdots & c &&& \\
  &&&&&& \ddots & \\
  &&&&&&& 1
 \end{pmatrix}
\end{equation}
That is a matrix where all diagonal elements are one except for the two elements $c$ is rows $p$ and $q$. All off-diagonal elements are zero except the two elements $s$ and $-s$ in positions $(p, q)$ and $(q, p)$.
We will choose $c, s$ to be the cosine and sine of a particular angle (so that $c^2 + s^2 = 1$) which we specify later.
Handily this means that $P_{pq}^{-1} = P_{pq}^T$.

We will apply the matrix $P_{pq}$ as a similarity transformation:
\begin{equation*}
A' = P_{pq}^{T} A P.
\end{equation*}

::: {#exm-one-jacobi-rotation}
Let $A$ be the $3 \times 3$ matrix given by
\begin{equation*}
 A = \begin{pmatrix}
 1 & 2 & \frac{3 \sqrt{2 - \sqrt{2}}}{4}\\2 & 5 & \frac{3 \sqrt{\sqrt{2} + 2}}{4}\\\frac{3 \sqrt{2 - \sqrt{2}}}{4} & \frac{3 \sqrt{\sqrt{2} + 2}}{4} & 2 \sqrt{2}
 \end{pmatrix}.
\end{equation*}

Let's apply the matrix as a similarity transform with $p=1, q=2$. Then we have
\begin{equation*}
 P_{1,2} = \begin{pmatrix}
 c & s & 0\\- s & c & 0\\0 & 0 & 1
 \end{pmatrix},
\end{equation*}
where $c = \cos(\theta)$ and $s = \sin(\theta)$ and $\theta$ is still to be determined.

We can compute that
\begin{equation*}
 A P_{1,2} =
 \begin{pmatrix}
 c - 2 s & 2 c + s & \frac{3 \sqrt{2 - \sqrt{2}}}{4}\\2 c - 5 s & 5 c + 2 s & \frac{3 \sqrt{\sqrt{2} + 2}}{4}\\\frac{3 c \sqrt{2 - \sqrt{2}}}{4} - \frac{3 s \sqrt{\sqrt{2} + 2}}{4} & \frac{3 c \sqrt{\sqrt{2} + 2}}{4} + \frac{3 s \sqrt{2 - \sqrt{2}}}{4} & 2 \sqrt{2}
 \end{pmatrix}
 \end{equation*}
 and
\begin{equation*}
 A' = P_{1, 2}^T A P_{1,2} = \begin{pmatrix}
 c^{2} - 4 c s + 5 s^{2} & 2 c^{2} - 4 c s - 2 s^{2} & \frac{3 c \sqrt{2 - \sqrt{2}}}{4} - \frac{3 s \sqrt{\sqrt{2} + 2}}{4}\\2 c^{2} - 4 c s - 2 s^{2} & 5 c^{2} + 4 c s + s^{2} & \frac{3 c \sqrt{\sqrt{2} + 2}}{4} + \frac{3 s \sqrt{2 - \sqrt{2}}}{4}\\\frac{3 c \sqrt{2 - \sqrt{2}}}{4} - \frac{3 s \sqrt{\sqrt{2} + 2}}{4} & \frac{3 c \sqrt{\sqrt{2} + 2}}{4} + \frac{3 s \sqrt{2 - \sqrt{2}}}{4} & 2 \sqrt{2}
 \end{pmatrix}
 \end{equation*}
Our grant aim is to nudge $A$ towards being diagonal.
Since, we've started with the $(1, 2)$ entry we might hope that we can make $A'_{1,2}$ to be zero. To do this we need $\theta$ such that
\begin{equation*}
2 \cos^{2}\theta - 4 \cos\theta \sin\theta - 2 \sin^2 \theta = 0
\end{equation*}
We can see that if $\theta = \pi/2 + m \pi$ (for some $m \in \mathbb{Z}$), $\cos\theta = 0$ but $\sin\theta = \pm 1$ and we don't have a solution. Otherwise, $\cos\theta \neq 0$ so we can divide by $- 2 \cos^2\theta$ to get
\begin{equation*}
\tan^2\theta + 2 \tan(\theta) - 1 = 0.
\end{equation*}
This is a quadratic equation in $\tan\theta$ with roots:
\begin{equation*}
\tan\theta = -1 \pm \sqrt{2}.
\end{equation*}
Taking the smaller root (with $+$ sign), we get (using some trigonometric identities):
\begin{equation}
\label{eq:trig_id}
\begin{aligned}
\tan\theta & = -1 + \sqrt{2} \\
\cos\theta & = 1 / \sqrt{\tan^2\theta +1} = \sqrt{\sqrt{2}/4 + 1/2} \\
\sin\theta & = \tan\theta \cos\theta
=  \sqrt{1/2 - \sqrt{2}/4}.
\end{aligned}
\end{equation}

Applying these substitutions, we arrive at
\begin{equation*}
A' = \begin{pmatrix}
3 - 2 \sqrt{2} & 0 & 0\\0 & 2 \sqrt{2} + 3 & \frac{3}{2}\\0 & \frac{3}{2} & 2 \sqrt{2}
\end{pmatrix}
\end{equation*}

::: {#exr-sim-trans}
Continue this exercise with $P_{2, 3}$.
Show that $\theta$ can be chosen so that
\begin{align*}
A'' = P_{2, 3}^{-1} A' P_{2, 3}
= \begin{pmatrix}
3 - 2 \sqrt{2} & 0 & 0\\0 & \frac{3}{2} + \frac{7 \sqrt{2}}{2} & 0\\0 & 0 & \frac{\sqrt{2}}{2} + \frac{3}{2}
\end{pmatrix} \\
\approx
\begin{pmatrix}
0.17157287525381 & 0 & 0\\0 & 6.44974746830583 & 0\\0 & 0 & 2.20710678118655
\end{pmatrix}.
\end{align*}
:::
:::

The general case follows in a similar way.
Multiplying out $A' = P_{pq}^{T} A P_{pq}$ for a general $n \times n$ matrix gives us the update formulae:
\begin{align*}
A'_{r, p} & = c A_{r, p} - s A_{r, q}
&& \text{for} \quad r \in \{1, \ldots n\} \setminus \{p, q\} \\
A'_{r, q} & = c A_{r, q} + s A_{r, p}
&& \text{for} \quad r \in \{1, \ldots n\} \setminus \{p, q\} \\
A'_{q, r} & = c A_{p, r} - s A_{q, r}
&& \text{for} \quad r \in \{1, \ldots n\} \setminus \{p, q\} \\
A'_{q, r} & = c A_{q, r} + s A_{p, r}
&& \text{for} \quad r \in \{1, \ldots n\} \setminus \{p, q\} \\
A'_{p, p} & = c^2 A_{p, p} + s^2 A_{q, q} - 2 c s A_{p, q} \\
A'_{q, q} & = s^2 A_{p, p} + c^2 A_{q, q} + 2 c s A_{p, q} \\
A'_{p, q} & = (c^2 - s^2) A_{p, q} + c s (A_{p, p} - A_{q, q}) \\
A'_{q, p} & = (c^2 - s^2) A_{q, p} + c s (A_{p, p} - A_{q, q}) \\
\end{align*}
The equation we need to solve is to find $\theta$ such that
\begin{equation*}
A'_{p, q} = (\cos^2\theta - \sin^2\theta) A_{p, q} + \cos\theta \sin\theta (A_{p, p} - A_{q, q}) = 0
\end{equation*}
As in the example, we see that $\pi/2 + m \pi$ is not a solution, so we are safe to assume that $\cos\theta \neq 0$. Further, since we want to set $A'_{p, q}$ to zero, we can assume that $A_{p, q} \neq 0$ (otherwise we don't need to address this pair of $p$ and $q$).
So we will divide by $-\cos^2\theta A_{p, q}$ to get a quadratic equation in $\tan\theta$:
\begin{equation*}
-1 + \tan^2\theta - \tan\theta \frac{A_{p, p} - A_{q, q}}{A_{p, q}} = 0
\end{equation*}
We follow the example above and take the smallest root using the quadratic formula and apply the same trigonometric identities in order to compute $c$ and $s$ \eqref{eq:trig_id}.

The only remaining step is to decide which order to cover $p$ and $q$. The update formula only needs to be applied to the upper triangular part of the matrix away from the diagonal.
In Jacobi's original algorithm from 1846, he proposed to eliminate each of the largest possible values in turn. This turns out to be computationally expensive and not necessary. Instead we simply iterate through all possible nonzero values until we have eliminated all off-diagonal values.

## Python code

```{python}
def jacobi_rotation(A, p, q):
    """
    Computes one iteration of P_{pq}^T A P_{pq}.
    Inputs: symmetric matrix A, indices p, q
    Output: symmetric matrix A' with A'_{p, q} = 0 (and A'_{q, p} = 0).
    """

    # copy matrix to new holder
    A_new = A.copy()

    # solve quadratic equation
    b = (A[q, q] - A[p, p]) / A[p, q]
    roots = (-b + np.sqrt(b * b + 4)) / 2, (-b - np.sqrt(b * b + 4)) / 2
    t = min(roots, key=abs)  # take smallest in absolute value root

    # determine c, s
    c = 1.0 / np.sqrt(t * t + 1.0)
    s = c * t

    # do updates
    A_new[:, p] = c * A[:, p] - s * A[:, q]
    A_new[:, q] = c * A[:, q] + s * A[:, p]
    A_new[p, :] = c * A[p, :] - s * A[q, :]
    A_new[q, :] = c * A[q, :] + s * A[p, :]

    A_new[p, p] = c**2 * A[p, p] + s**2 * A[q, q] - 2 * c * s * A[p, q]
    A_new[q, q] = s**2 * A[p, p] + c**2 * A[q, q] + 2 * c * s * A[p, q]

    A_new[p, q] = 0.0  # replace update formula with exact value
    A_new[q, p] = 0.0  # replace update formula with exact value

    return A_new


```

Let's try it out!

```{python}

A = np.array(
    [
        [1, 2, 3 * np.sqrt(2 - np.sqrt(2)) / 4],
        [2, 5, 3 * np.sqrt(2 + np.sqrt(2)) / 4],
        [
            3 * np.sqrt(2 - np.sqrt(2)) / 4,
            3 * np.sqrt(2 + np.sqrt(2)) / 4,
            2 * np.sqrt(2),
        ],
    ]
)
np_ev, _ = np.linalg.eig(A)

print("initial matrix")
print(A)

A = jacobi_rotation(A, 0, 1)

print("after one rotation (0, 1)")
print(A)

A = jacobi_rotation(A, 1, 2)

print("after second rotation (1, 2)")
print(A)

print("Our estimate of the eigenvalues is")
print(np.diag(A))
print("Numpy's estimate of the eigenvalues is")
print(np_ev)
```

Let's try again with a harder problem

```{python}

n = 10
tol = 1.0e-12

# generate a random matrix
np.random.seed(42)
S = special_ortho_group.rvs(n)
D = np.diag(np.random.randint(-5, 5, (n,)))
A = S.T @ D @ S

print("initial matrix A")
print(A)

np_ev, _ = np.linalg.eig(A)

# sweep over matrix several times
for sweep in range(10):
    for i in range(n):
        for j in range(i + 1, n):
            if abs(A[i, j]) < tol:
                continue

            A = jacobi_rotation(A, i, j)

    non_diag_nonzeros = 0
    non_diag_zeros = 0
    for i in range(n):
        for j in range(n):
            if i == j:
                continue
            if abs(A[i, j]) < tol:
                non_diag_zeros += 1
            else:
                non_diag_nonzeros += 1

    print(f"end of sweep {sweep}: {non_diag_zeros=} {non_diag_nonzeros=}")
    if non_diag_nonzeros == 0:
        break

print("our estimate of the eigenvalues is")
print(np.diag(A))
print("Numpy's estimate of the eigenvalues is")
print(np_ev)
```
We see that it takes 10 sweeps to find all the eigenvalues!

## Computing eigenvectors

After enough sweeps, we end up with a diagonal matrix (at least to machine precision), let's call it $D$. But indeed, we have computed a factorisation:
\begin{equation*}
D = V^T A V,
\end{equation*}
where $V = P_1 P_2 \cdots$ and each $P_j$ is a Jacobi rotation matrix. We can immediately identify the matrix $V$ as having columns which correspond to the eigenvectors of $A$.

The eigenvector matrix can be computed by successively computing:
\begin{equation*}
V' = V P_{pq},
\end{equation*}
starting from $V$ is the identity matrix.
We can see that componentwise, we have the update formulae:
\begin{align*}
v'_{rs} &= v_{rs} && s \neq p, s \neq q \\
v'_{rp} &= c v_{rp} - s v_{rq} && ?? \\
v'_{rq} &= s v_{rp} + c v_{rq} && ??.
\end{align*}

::: {#exm-one-jacobi-rotation2}
Continuing @exm-one-jacobi-rotation. We computed that in the first iteration
\begin{align*}
\cos\theta & = 1 / \sqrt{\tan^2\theta +1} = \sqrt{\sqrt{2}/4 + 1/2} \\
\sin\theta & = \tan\theta \cos\theta
=  \sqrt{1/2 - \sqrt{2}/4}.
\end{align*}

Starting from
$$
V = \begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix},
$$
we can compute $V'$ as
$$
V' = \begin{pmatrix}
c & s & 0 \\
-s & c & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
= \begin{pmatrix}
\sqrt{\sqrt{2}/4 + 1/2} & \sqrt{1/2 - \sqrt{2}/4} & 0 \\
-\sqrt{1/2 - \sqrt{2}/4} & \sqrt{\sqrt{2}/4 + 1/2} & 0 \\
0 & 0 & 1 \\
\end{pmatrix}.
$$

:::: {#exr-sim-trans2}
Continue this exercise with $P_{2, 3}$ to show
\begin{align*}
& V'' = V' P_{2, 3}
= \begin{pmatrix}
\frac{\sqrt{\sqrt{2} + 2}}{2} & - \frac{\sqrt{2}}{4} & \frac{1}{2} - \frac{\sqrt{2}}{4}\\- \frac{\sqrt{2 - \sqrt{2}}}{2} & - \frac{1}{2} - \frac{\sqrt{2}}{4} & \frac{\sqrt{2}}{4}\\0 & - \frac{\sqrt{2 - \sqrt{2}}}{2} & - \frac{\sqrt{\sqrt{2} + 2}}{2}
\end{pmatrix} \\
& \approx
\begin{pmatrix}
0.923879532511287 & -0.353553390593274 & 0.146446609406726\\-0.38268343236509 & -0.853553390593274 & 0.353553390593274\\0 & -0.38268343236509 & -0.923879532511287\end{pmatrix}.
\end{align*}
:::
:::

## Python code (again)

We can try our methods on
```{python}


def jacobi_rotation_with_V(A, V, p, q):
    """
    Computes one iteration of P_{pq}^T A P_{pq} and V P_{pq}
    Inputs: symmetric matrix A, matrix V, indices p, q
    Output: symmetric matrix A' with A'_{p, q} = 0 (and A'_{q, p} = 0), updated corresponding eigenvector matrix V
    """

    # copy matrix to new holder
    A_new = A.copy()
    V_new = V.copy()

    # solve quadratic equation
    b = (A[q, q] - A[p, p]) / A[p, q]
    roots = (-b + np.sqrt(b * b + 4)) / 2, (-b - np.sqrt(b * b + 4)) / 2
    t = min(roots, key=abs)  # take smallest in absolute value root

    # determine c, s
    c = 1.0 / np.sqrt(t * t + 1.0)
    s = c * t

    # do updates
    A_new[:, p] = c * A[:, p] - s * A[:, q]
    A_new[:, q] = c * A[:, q] + s * A[:, p]
    A_new[p, :] = c * A[p, :] - s * A[q, :]
    A_new[q, :] = c * A[q, :] + s * A[p, :]

    A_new[p, p] = c**2 * A[p, p] + s**2 * A[q, q] - 2 * c * s * A[p, q]
    A_new[q, q] = s**2 * A[p, p] + c**2 * A[q, q] + 2 * c * s * A[p, q]

    A_new[p, q] = 0.0  # replace update formula with exact value
    A_new[q, p] = 0.0  # replace update formula with exact value

    V_new[:, p] = c * V[:, p] - s * V[:, q]
    V_new[:, q] = s * V[:, p] + c * V[:, q]

    return A_new, V_new


```

Let's try it out!

```{python}
A = np.array(
    [
        [1, 2, 3 * np.sqrt(2 - np.sqrt(2)) / 4],
        [2, 5, 3 * np.sqrt(2 + np.sqrt(2)) / 4],
        [
            3 * np.sqrt(2 - np.sqrt(2)) / 4,
            3 * np.sqrt(2 + np.sqrt(2)) / 4,
            2 * np.sqrt(2),
        ],
    ]
)
np_eval, np_evec = np.linalg.eig(A)

V = np.eye(A.shape[0])

print("initial matrices")
print(A)
print(V)

A, V = jacobi_rotation_with_V(A, V, 0, 1)

print("after one rotation (0, 1)")
print(A)
print(V)

A, V = jacobi_rotation_with_V(A, V, 1, 2)

print("after second rotation (1, 2)")
print(A)
print(V)

print("Our estimate of the eigenvalues is")
print(np.diag(A))
print("Numpy's estimate of the eigenvalues is")
print(np_eval)

print("Our estimate of eigenvectors is")
print(V)
print("Numpy's estimate of the eigenvalues is")
print(np_evec)
```

Success! We have the same values (up to reordering).

We can also apply the same code to the larger problem size:

```{python}
n = 10
tol = 1.0e-12

# generate a random matrix
np.random.seed(42)
S = special_ortho_group.rvs(n)
D = np.diag(np.random.randint(-5, 5, (n,)))
A = S.T @ D @ S
V = np.eye(n)

print("initial matrix A")
print(A)

np_eval, np_evec = np.linalg.eig(A)

# sweep over matrix several times
for sweep in range(10):
    for i in range(n):
        for j in range(i + 1, n):
            if abs(A[i, j]) < tol:
                continue

            A, V = jacobi_rotation_with_V(A, V, i, j)

    non_diag_nonzeros = 0
    non_diag_zeros = 0
    for i in range(n):
        for j in range(n):
            if i == j:
                continue
            if abs(A[i, j]) < tol:
                non_diag_zeros += 1
            else:
                non_diag_nonzeros += 1

    print(f"end of sweep {sweep}: {non_diag_zeros=} {non_diag_nonzeros=}")
    if non_diag_nonzeros == 0:
        break

print("our estimate of the eigenvalues is")
print(np.diag(A))
print("Numpy's estimate of the eigenvalues is")
print(np_ev)

print("Our estimate of eigenvectors is")
print(V)
print("Numpy's estimate of the eigenvalues is")
print(np_evec)
```

## Computational cost

A quick look at the cost of each rotation shows that the main cost is in updating all the entries. For a matrix of size $n$, we must update $4n-4$ entries (two rows and two columns) in $A$ and $2n$ entries in (two rows) in $V$. We say that each rotation costs $O(n)$.

One sweep corresponds to trying to eliminate all off diagonal entries (i.e. the strictly upper triangular portion of the matrix). So each sweep corresponds to $n (n-1)/2$ rotations or $(6n - 4) n (n-1) / 2$. So each sweep is $O(n^3)$.

The remaining question is how many sweeps are required? Let's test this using our code.

```{python}
# | echo: false

tol = 1e-12
max_sweeps = 100

n_values = []
sweep_values = []
times = []

for n in range(10, 100, 5):
    times_holder_n = []
    sweeps_holder_n = []

    for _ in range(5):
        S = special_ortho_group.rvs(n)
        D = np.diag(np.random.randint(-5, 5, (n,)))
        A = S.T @ D @ S
        V = np.eye(n)

        my_time = 0.0

        # sweep over matrix several times
        for sweep in range(max_sweeps):
            start = time.time()
            for i in range(n):
                for j in range(i + 1, n):
                    if abs(A[i, j]) < tol:
                        continue

                    A, V = jacobi_rotation_with_V(A, V, i, j)

            end = time.time()
            my_time += end - start

            non_diag_nonzeros = 0
            non_diag_zeros = 0
            for i in range(n):
                for j in range(n):
                    if i == j:
                        continue
                    if abs(A[i, j]) < tol:
                        non_diag_zeros += 1
                    else:
                        non_diag_nonzeros += 1

            if non_diag_nonzeros == 0:
                n_sweeps = sweep + 1
                break

        times_holder_n.append(my_time)
        sweeps_holder_n.append(n_sweeps)

    n_values.append(n)
    sweep_values.append(np.average(sweeps_holder_n))
    times.append(np.average(times_holder_n))
```

```{python}
# | echo: false

n_values = np.array(n_values)


def sweep_fit_func(n, *params):
    a, b = params
    return a * n**0.5 + b


opt, cov = curve_fit(sweep_fit_func, n_values, sweep_values, (1, 0))

plt.plot(n_values, sweep_values, "o", label="samples")
if len(opt) == 1:
    raise Exception(opt)
plt.plot(
    n_values,
    sweep_fit_func(n_values, *opt),
    label=f"fit  ({opt[0]:.2f} $n^{{1/2}}$ + {opt[1]:.2f})",
)

plt.xlabel("$n$")
plt.ylabel("no. of sweeps")
plt.legend()

plt.grid(True)
plt.show()
```

This gives us an estimate that the number of sweeps is $O(n^{1/2})$.
So we would expect the time to run the algorithm to scale like $n^{7/2}$.
However, in practical situations, we see that the growth is closer to $n^{5/2}$:

```{python}
# | echo: false


def time_fit_func(log_n, *params):
    a, b = params
    return a * log_n + b


def time_fit_func_rescale(n, *params):
    a, b = params
    return np.exp(b) * n**a


opt, cov = curve_fit(time_fit_func, np.log(n_values), np.log(times), p0=(1, 0))

plt.loglog(n_values, times, "o", label="samples")
plt.loglog(
    n_values,
    time_fit_func_rescale(n_values, *opt),
    label=f"fit ($({np.exp(opt[1]):.2e}) n^{{{opt[0]:.2f}}})$",
)

plt.xlabel("$n$")
plt.ylabel("time")
plt.legend()

plt.grid(True)
plt.show()
```

## References

Numerical recipes, chapter XX?
