# Lecture 04: Gaussian elimination

## Elementary row operations

Recall the problem is to solve a set of $n$ **linear** equations for $n$ unknown values $x_j$, for $j=1, 2, \ldots, n$.

**Notation**:

$$
\begin{aligned}
\text{Equation } 1: && a_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \cdots + a_{1n} x_n & = b_1 \\
\text{Equation } 2: && a_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \cdots + a_{2n} x_n & = b_2 \\
\vdots \\
\text{Equation } i: && a_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \cdots + a_{in} x_n & = b_i \\
\vdots \\
\text{Equation } n: && a_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \cdots + a_{nn} x_n & = b_n.
\end{aligned}
$$

### Elementary row operations

Consider equation $p$ of the above system:

$$
a_{p1} x_1 + a_{p2} x_2 + a_{p3} x_3 + \cdots + a_{pn} x_n = b_p,
$$

and equation $q$:

$$
a_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \cdots + a_{qn} x_n = b_q.
$$

Note three things...

-   The order in which we choose to write the $n$ equations is irrelevant

-   We can multiply any equation by an arbitrary real number ($k \neq 0$ say):

    $$
    k a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \cdots + k a_{pn} x_n = k b_p.
    $$

-   We can add any two equations:

    $$
    k a_{p1} x_1 + k a_{p2} x_2 + k a_{p3} x_3 + \cdots + k a_{pn} x_n = k b_p
    $$

	added to

	$$
    a_{q1} x_1 + a_{q2} x_2 + a_{q3} x_3 + \cdots + a_{qn} x_n = b_q
    $$

	yields

	$$
    (k a_{p1} + a_{q1}) x_1 + (k a_{p2} + a_{q2}) x_2 + \cdots + (k a_{pn} + a_{qn}) x_n = k b_p + b_q.
    $$

### Example 1

Consider the system

$$
\begin{aligned}
2 x_1 + 3 x_2 & = 4 && (1) \\
-3 x_1 + 2 x_2 & = 7 && (2).
\end{aligned}
$$

-   $4 \times (1)$ $\rightarrow$ $8 x_1 + 12 x_2 = 16$.
-   $-1.5 \times (2)$ $\rightarrow$ $4.5 x_1 - 3 x_2 = -10.5$.
-   $(2) + (1)$ $\rightarrow$ $-x_1 + 5 x_2 = 11$.
-   $(2) + 1.5 \times (1)$ $\rightarrow$ $0 + 6.5 x_2 = 13$.

### Example 2 (homework)

Consider the system

$$
\begin{aligned}
x_1 + 2 x_2 & = 1 && (3) \\
4 x_1 + x_2 & = -3 && (4).
\end{aligned}
$$

-   $2 \times (3)$ $\rightarrow$
-   $0.25 \times (4)$ $\rightarrow$
-   $(4) + (-1) \times (3)$ $\rightarrow$
-   $(4) + (-4) \times (3)$ $\rightarrow$

### General matrix-vector form

What does this mean when we write the equations as a single matrix equation?

$$
 \begin{pmatrix}
 a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
 a_{21} & a_{22} & a_{23} & \cdots & a_{2n} \\
 a_{31} & a_{32} & a_{33} & \cdots & a_{3n} \\
 \vdots & \vdots & \vdots & & \vdots \\
 a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn}
 \end{pmatrix}
 \begin{pmatrix}
 x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n
 \end{pmatrix} =
 \begin{pmatrix}
 b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_n
 \end{pmatrix}.
$$

Recall the $n \times n$ matrix $A$ represents the coefficients that multiply the unknowns in each equation (row), while the $n$-vector $\vec{b}$ represents the right-hand-side values.

### Application to matrix equations

For a system written in matrix form our three observations mean the following:

-   We can swap any two rows of the matrix (and corresponding right-hand side entries). For example:

    $$
    \begin{pmatrix}
    2 & 3 \\ -3 & 2
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2
    \end{pmatrix} =
    \begin{pmatrix}
    4 \\ 7
    \end{pmatrix}
    \Rightarrow
    \begin{pmatrix}
    -3 & 2\\ 2 & 3
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2
    \end{pmatrix} =
    \begin{pmatrix}
    7 \\ 4
    \end{pmatrix}
    $$

-   We can multiply any row of the matrix (and corresponding right-hand side entry) by a scalar. For example:

    $$
    \begin{pmatrix}
    2 & 3 \\ -3 & 2
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2
    \end{pmatrix} =
    \begin{pmatrix}
    4 \\ 7
    \end{pmatrix}
    \Rightarrow
    \begin{pmatrix}
    1 & \frac{3}{2} \\ -3 & 2
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2
    \end{pmatrix} =
    \begin{pmatrix}
    2 \\ 7
    \end{pmatrix}
    $$

-   We can replace row $q$ by row $q + k \times$ row $p$. For example:

	$$
    \begin{pmatrix}
    2 & 3 \\ -3 & 2
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2
    \end{pmatrix} =
    \begin{pmatrix}
    4 \\ 7
    \end{pmatrix}
    \Rightarrow
    \begin{pmatrix}
    2 & 3 \\ 0 & 6.5
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2
    \end{pmatrix} =
    \begin{pmatrix}
    4 \\ 13
    \end{pmatrix}
    $$

	(here we replaced row $w$ by row $2 + 1.5 \times$ row $1$)

    $$
    \begin{pmatrix}
    1 & 2 \\ 4 & 1
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2
    \end{pmatrix} =
    \begin{pmatrix}
    1 \\ -3
    \end{pmatrix}
    \Rightarrow
    \begin{pmatrix}
    1 & 2 \\ 0 & -7
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2
    \end{pmatrix} =
    \begin{pmatrix}
    1 \\ -7
    \end{pmatrix}
    $$

	(here we replaced row $2$ by row $2 + (-4) \times$ row $1$)

### Triangular systems

Recall that we can easily solve a lower triangular system:

$$
 \begin{pmatrix}
 a_{11} & 0 & 0 & \cdots & 0 \\
 a_{21} & a_{22} & 0 & \cdots & 0 \\
 a_{31} & a_{32} & a_{33} & \cdots & 0 \\
 \vdots & \vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn}
 \end{pmatrix}
 \begin{pmatrix}
 x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n
 \end{pmatrix} =
 \begin{pmatrix}
 b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_n
 \end{pmatrix}.
$$

-   $A$ is a lower triangular matrix if every entry above the leading diagonal is zero

    $$
    a_{ij} = 0 \text{ for } j > i.
    $$

Similarly, we can easily solve an upper triangular system:

$$
 \begin{pmatrix}
 a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
 0 & a_{22} & a_{23} & \cdots & a_{2n} \\
 0 & 0 & a_{33} & \cdots & a_{3n} \\
 \vdots & \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & 0 & \cdots & a_{nn}
 \end{pmatrix}
 \begin{pmatrix}
 x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n
 \end{pmatrix} =
 \begin{pmatrix}
 b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_n
 \end{pmatrix}.
$$

-   $A$ is an upper triangular matrix if every entry below the leading diagonal is zero

    $$
    a_{ij} = 0 \text{ for } i > j.
    $$

### Strategy

-   Three types of operation described above are called **elementary row operations** (ERO).

-   We know that an upper triangular system can be easily solved... so, can we systematically apply a sequence of ERO to reduce an arbitrary system to triangular form?

-   The answer is "yes"... and the algorithm for doing this is known as **forward elimination** or (more commonly) as **Gaussian elimination** (GE).

## Gaussian elimination

**First appeared in China...**

From [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_elimination):

> The method of Gaussian elimination appears in the Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 CE, but parts of it were written as early as approximately 150 BCE. It was commented on by Liu Hui in the 3rd century.
>
> The method in Europe stems from the notes of Isaac Newton. In 1670, he wrote that all the algebra books known to him lacked a lesson for solving simultaneous equations, which Newton then supplied. Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the normal equations of least-squares problems. The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.

```{figure} ../img/lec06/NineChapters.jpg
Image from Nine Chapter of the Mathematical art
```

### The algorithm

The following algorithm systematically introduces zeros into the system of equations, below the diagonal.

1.  Subtract multiples of row 1 from the rows below it to eliminate (make zero) nonzero entries in column 1.
2.  Subtract multiplies of the new row 2 from the rows below it to eliminate nonzero entries in column 2.
3.  Repeat for row $3, 4, \ldots, n-1$.

After row $n-1$ all entities below the diagonal have been eliminated, so $A$ is now upper triangular and the resulting system can be solved by backward substitution.

### Example 1

Use Gaussian eliminate to solve the linear system of equations given by

$$
\begin{pmatrix}
 2 & 1 & 4 \\ 1 & 2 & 2 \\ 2 & 4 & 6
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix} =
\begin{pmatrix}
12 \\ 9 \\ 22
\end{pmatrix}.
$$

First, use the first row to eliminate the first column below the diagonal:

-   (row 2) $- 0.5 \times$ (row 1) gives

    $$
    \begin{pmatrix}
     2 & 1 & 4 \\ \mathbf{0} & 1.5 & 0 \\ 2 & 4 & 6
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2 \\ x_3
    \end{pmatrix} =
    \begin{pmatrix}
    12 \\ 3 \\ 22
    \end{pmatrix}
    $$

-   (row 3) $-$ (row 1) then gives

    $$
    \begin{pmatrix}
     2 & 1 & 4 \\ \mathbf{0} & 1.5 & 0 \\ \mathbf{0} & 3 & 2
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2 \\ x_3
    \end{pmatrix} =
    \begin{pmatrix}
    12 \\ 3 \\ 10
    \end{pmatrix}
    $$

Now use the second row to eliminate the second column below the diagonal.

-   (row 3) $- 2 \times$ (row 2) gives

    $$
    \begin{pmatrix}
     2 & 1 & 4 \\ \mathbf{0} & 1.5 & 0 \\ \mathbf{0} & \mathbf{0} & 2
    \end{pmatrix}
    \begin{pmatrix}
    x_1 \\ x_2 \\ x_3
    \end{pmatrix} =
    \begin{pmatrix}
    12 \\ 3 \\ 4
    \end{pmatrix}
    $$

The system is now in upper triangular form and can be solved using backward substitution to give $\vec{x} = (1, 2, 2)^T$ (see the [final example from previous lecture](lec05.md)).

### Example 2 (homework)

Use Gaussian elimination to solve the linear system of equations given by

$$
\begin{pmatrix}
4 & -1 & -1 \\ 2 & 4 & 2 \\ 1 & 2 & 4
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix} =
\begin{pmatrix}
9 \\ -6 \\ 3
\end{pmatrix}.
$$

### Notes

-   Each row $i$ is used to eliminate the entries in column $i$ below $a_{ii}$, i.e.Â it forces $a_{ji} = 0$ for $j > i$.

-   This is done by subtracting a multiple of row $i$ from row $j$:

    $$
    (\text{row } j) \leftarrow (\text{row } j) - \frac{a_{ji}}{a_{ii}} (\text{row } i).
    $$

-   This guarantees that $a_{ji}$ becomes zero because

    $$
    a_{ji} \leftarrow a_{ji} - \frac{a_{ji}}{a_{ii}} a_{ii} = a_{ji} - a_{ji} = 0.
    $$

### Example 3 (homework)

Solve the system

$$
\begin{pmatrix}
4 & 3 & 2 & 1 \\ 1 & 2 & 2 & 2 \\
1 & 1 & 3 & 0 \\ 2 & 1 & 2 & 3
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4
\end{pmatrix} =
\begin{pmatrix}
 10 \\ 7 \\ 5 \\ 8
\end{pmatrix}.
$$

The solution is $\vec{x} = (1, 1, 1, 1)^T$.

## Final Notes

GE can be used to solve linear systems of equations...

-   The computational cost is $O(n^3)$ -- which can be quite high for large values of $n$.

-   Problems can occur if $\,a_{ii} = 0\,$ for any $i$ in a row that is being used to do the elimination;

-   Can overcome by swapping row $i$ with any row beneath (more later).

-   If $\,a_{ii} = 0\,$ for a row that is being used to do the elimination, and all rows beneath have a zero in column $i$, then the GE algorithm breaks down.

- The GE algorithm will only break down if the system is **singular**  (so no solution exists or no unique solution).

- If the unique solution does exist the GE algorithm (with row swapping if needed)  will find it!


##  The cost of Gaussian Elimination

-   Gaussian elimination (GE) is unnecessarily expensive when it is applied to many systems of equations with the same matrix $A$ but different right-hand sides $\vec{b}$.

    -   The forward elimination process is the most computationally expensive part at $O(n^3)$ but is exactly the same for any choice of $\vec{b}$.
    -   In contrast, the solution of the resulting upper triangular system only requires $O(n^2)$ operations.

-   We can use this information to improve the way in which we solve multiple systems of equations with the same matrix $A$ but different right-hand sides $\vec{b}$.

### Elementary row operations (EROs)

Note that the EROs discussed in the last lecture can be produced by left multiplication with a suitable matrix:

-   Row swap:

    $$
    \begin{pmatrix}
    1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0
    \end{pmatrix}
    \begin{pmatrix}
    a & b & c \\ d & e & f \\ g & h & i
    \end{pmatrix}
    =
    \begin{pmatrix}
    a & b & c \\ g & h & i \\ d & e & f
    \end{pmatrix}
    $$

-   Row swap:

    $$
    \begin{pmatrix}
    1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
    a & b & c & d \\ e & f & g & h \\ i & j & k & l \\ m & n & o & p
    \end{pmatrix}
    =
    \begin{pmatrix}
    a & b & c & d \\ i & j & k & l \\ e & f & g & h \\ m & n & o & p
    \end{pmatrix}
    $$

-   Multiply row by $\alpha$:

    $$
    \begin{pmatrix}
    \alpha & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
    a & b & c \\ d & e & f \\ g & h & i
    \end{pmatrix}
    =
    \begin{pmatrix}
    \alpha a & \alpha b & \alpha c \\ d & e & f \\ g & h & i
    \end{pmatrix}
    $$

-   $\alpha \times \text{row } p + \text{row } q$:

    $$
    \begin{pmatrix}
    1 & 0 & 0 \\ 0 & 1 & 0 \\ \alpha & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
    a & b & c \\ d & e & f \\ g & h & i
    \end{pmatrix}
    =
    \begin{pmatrix}
    a & b & c \\ d & e & f \\ \alpha a + g & \alpha b + h & \alpha c + i
    \end{pmatrix}
    $$

## LU factorisation

-   Recall from the last lecture that Gaussian elimination (GE) is just a sequence of EROs.

-   Each of these EROs is equivalent to (left) multiplication by a suitable matrix, $E$ say.

-   Hence, forward elimination applied to the system $A \vec{x} = \vec{b}$ can be expressed as

    $$
    (1) \qquad
    (E_m \cdots E_1) A \vec{x} = (E_m \cdots E_1) \vec{b},
    $$

	where $m$ is the number of EROs required to reduce the upper triangular form.

-   Let $U = (E_m \cdots E_1) A$ and $L = (E_m \cdots E_1)^{-1}$.

-   Now the original system $A \vec{x} = \vec{b}$ is equivalent to

    $$
    (2) \qquad
    L U \vec{x} = \vec{b}
    $$

	where $U$ is *upper triangular* (by construction) and $L$ may be shown to be lower triangular (provided the EROs do not include any row swaps).

-   Once $L$ and $U$ are known it is easy to solve $(2)$:

    -   Solve $L \vec{z} = \vec{b}$ in $O(n^2)$ operations.
    -   Solve $U \vec{x} = \vec{z}$ in $O(n^2)$ operations.

-   $L$ and $U$ may be found in $O(n^3)$ operations by performing GE and saving the $E_i$ matrices, however it is more convenient to find them directly (also $O(n^3)$ operations).

### Computing $L$ and $U$

Consider a general $4 \times 4$ matrix $A$ and its factorisation $LU$:

$$
\begin{pmatrix}
a_{11} & a_{12} & a_{13} & a_{14} \\
a_{21} & a_{22} & a_{23} & a_{24} \\
a_{31} & a_{32} & a_{33} & a_{34} \\
a_{41} & a_{42} & a_{43} & a_{44}
\end{pmatrix}
 =
\begin{pmatrix}
1 & 0 & 0 & 0 \\
l_{21} & 1 & 0 & 0 \\
l_{31} & l_{32} & 1 & 0 \\
l_{41} & l_{42} & l_{43} & 1
\end{pmatrix}
\begin{pmatrix}
u_{11} & u_{12} & u_{13} & u_{14} \\
0 & u_{22} & u_{23} & u_{24} \\
0 & 0 & u_{33} & u_{34} \\
0 & 0 & 0 & u_{44}
\end{pmatrix}
$$

For the first column,

$$
\begin{aligned}
a_{11} & = (1, 0, 0, 0) (u_{11}, 0, 0, 0)^T && = u_{11}
 & \rightarrow u_{11} & = a_{11} \\
a_{21} & = (l_{21}, 1, 0, 0)(u_{11}, 0, 0, 0)^T && = l_{21} u_{11}
 & \rightarrow l_{21} & = a_{21} / u_{11} \\
a_{31} & = (l_{31}, l_{32}, 1, 0)(u_{11}, 0, 0, 0)^T && = l_{31} u_{11}
 & \rightarrow l_{31} & = a_{31} / u_{11} \\
a_{41} & = (l_{41}, l_{42}, l_{43}, 1)(u_{11}, 0, 0, 0)^T && = l_{41} u_{11}
 & \rightarrow l_{41} & = a_{41} / u_{11}
\end{aligned}
$$

The second, third and fourth columns follow in a similar manner, giving all the entries in $L$ and $U$.

### Notes

-   $L$ is assumed to have 1's on the diagonal, to ensure that the factorisation is unique.

-   The process involves division by the diagonal entries $u_{11}, u_{22}$, etc., so they **must** be non-zero.

-   In general the factors $l_{ij}$ and $u_{ij}$ are calculated for each column $j$ in turn, i.e.,

    ``` python
    for j in range(n):
      for i in range(j+1):
          # Compute factors u_{ij}
          ...
      for i in range(j+1, n):
          # Compute factors l_{ij}
          ...
    ```

### Example 1

Use $LU$ factorisation to solve the linear system of equations given by

$$
\begin{pmatrix}
2 & 1 & 4 \\
1 & 2 & 2 \\
2 & 4 & 6
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
12 \\ 9 \\ 22
\end{pmatrix}.
$$

This can be rewritten in the form $A = LU$ where

$$
\begin{pmatrix}
2 & 1 & 4 \\
1 & 2 & 2 \\
2 & 4 & 6
\end{pmatrix}
 =
\begin{pmatrix}
1 & 0 & 0 \\
l_{21} & 1 & 0 \\
l_{31} & l_{32} & 1
\end{pmatrix}
\begin{pmatrix}
u_{11} & u_{12} & u_{13} \\
0 & u_{22} & u_{23} \\
0 & 0 & u_{33}
\end{pmatrix}.
$$

Column 1 of $A$ gives

$$
\begin{aligned}
2 & = u_{11} && \rightarrow & u_{11} & = 2 \\
1 & = l_{21} u_{11} && \rightarrow & l_{21} & = 0.5 \\
2 & = l_{31} u_{11} && \rightarrow & l_{31} & = 1.
\end{aligned}
$$

Column 2 of $A$ gives

$$
\begin{aligned}
1 & = u_{12} && \rightarrow & u_{12} & = 1 \\
2 & = l_{21} u_{12} + u_{22} && \rightarrow & u_{22} & = 1.5 \\
4 & = l_{31} u_{12} + l_{32} u_{22} && \rightarrow & l_{32} & = 2.
\end{aligned}
$$

Column 3 of $A$ gives

$$
\begin{aligned}
4 & = u_{13} && \rightarrow & u_{13} & = 4 \\
2 & = l_{21} u_{13} + u_{23} && \rightarrow & u_{23} & = 0 \\
6 & = l_{31} u_{13} + l_{32} u_{23} + u_{33} && \rightarrow & u_{33} & = 2.
\end{aligned}
$$

Solve the lower triangular system $L \vec{z} = \vec{b}$:

$$
\begin{pmatrix}
1 & 0 & 0 \\
0.5 & 1 & 0 \\
1 & 2 & 1
\end{pmatrix}
\begin{pmatrix}
z_1 \\ z_2 \\ z_3
\end{pmatrix}
 =
\begin{pmatrix}
12 \\ 9 \\ 22
\end{pmatrix}
\rightarrow
\begin{pmatrix}
z_1 \\ z_2 \\ z_3
\end{pmatrix}
 =
\begin{pmatrix}
12 \\ 3 \\ 4
\end{pmatrix}
$$

Solve the upper triangular system $U \vec{x} = \vec{z}$:

$$
\begin{pmatrix}
2 & 1 & 4 \\
0 & 1.5 & 0 \\
0 & 0 & 2
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
\begin{pmatrix}
12 \\ 3 \\ 4
\end{pmatrix}
\rightarrow
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
1 \\ 2 \\ 2
\end{pmatrix}.
$$

### Example 2 (homework)

Rewrite the matrix $A$ as the product of lower and upper triangular matrices where

$$
A =
\begin{pmatrix}
4 & 2 & 0 \\
2 & 3 & 1 \\
0 & 1 & 2.5
\end{pmatrix}.
$$

### The link

The first example gives

$$
\begin{pmatrix}
2 & 1 & 4 \\
1 & 2 & 2 \\
2 & 4 & 6
\end{pmatrix}
 =
\begin{pmatrix}
 1 & 0 & 0 \\
 0.5 & 1 & 0 \\
 1 & 2 & 1
\end{pmatrix}
\begin{pmatrix}
 2 & 1 & 4 \\
 0 & 1.5 & 0 \\
 0 & 0 & 2
\end{pmatrix}
$$

Note that

-   the matrix $U$ is the same as the fully eliminated upper triangular form produced by Gaussian elimination;

-   $L$ contains the multipliers that were used at each stage to eliminate the rows.

# Lecture 10: Effects of finite precision arithmetic

## The need for row swapping in GE

Consider the following linear system of equations

$$
\begin{pmatrix}
0 & 2 & 1 \\
2 & 1 & 0 \\
1 & 2 & 0
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
7 \\ 4 \\ 5
\end{pmatrix}
$$

*Problem*. We cannot eliminate the first column by the diagonal by adding multiples of row 1 to rows 2 and 3 respectively.

### Row swapping

*Solution*. Swap the order of the equations!

- Swap rows 1 and 2:

  $$
  \begin{pmatrix}
  2 & 1 & 0 \\
  0 & 2 & 1 \\
  1 & 2 & 0
  \end{pmatrix}
  \begin{pmatrix}
  x_1 \\ x_2 \\ x_3
  \end{pmatrix}
  =
  \begin{pmatrix}
  4 \\ 7 \\ 5
  \end{pmatrix}
  $$

- Now apply Gaussian elimination

  $$
  \begin{pmatrix}
  2 & 1 & 0 \\
  0 & 2 & 1 \\
  0 & 1.5 & 0
  \end{pmatrix}
  \begin{pmatrix}
  x_1 \\ x_2 \\ x_3
  \end{pmatrix}
  =
  \begin{pmatrix}
  4 \\ 7 \\ 3
  \end{pmatrix}
  ;
  \begin{pmatrix}
  2 & 1 & 0 \\
  0 & 2 & 1 \\
  0 & 0 & -0.75
  \end{pmatrix}
  \begin{pmatrix}
  x_1 \\ x_2 \\ x_3
  \end{pmatrix}
  =
  \begin{pmatrix}
  4 \\ 7 \\ -2.25
  \end{pmatrix}.
  $$

### Another example

Consider another system of equations

$$
\begin{pmatrix}
2 & 1 & 1 \\
4 & 2 & 1 \\
2 & 2 & 0
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
 3 \\ 5 \\ 2
\end{pmatrix}
$$

- Apply Gaussian elimination as usual:

  $$
  \begin{pmatrix}
  2 & 1 & 1 \\
  0 & 0 & -1 \\
  2 & 2 & 0
  \end{pmatrix}
  \begin{pmatrix}
  x_1 \\ x_2 \\ x_3
  \end{pmatrix}
  =
  \begin{pmatrix}
  3 \\ -1 \\ 2
  \end{pmatrix}
  ;
  \begin{pmatrix}
  2 & 1 & 1 \\
  0 & 0 & -1 \\
  0 & 1 & -1
  \end{pmatrix}
  \begin{pmatrix}
  x_1 \\ x_2 \\ x_3
  \end{pmatrix}
  =
  \begin{pmatrix}
  3 \\ -1 \\ -1
  \end{pmatrix}
  $$

- *Problem*. We cannot eliminate the second column below the diagonal by adding a multiple of row 2 to row3.

- Again this problem may be overcome simply by swapping the order of the equations - this time swapping rows 2 and 3:

  $$
  \begin{pmatrix}
  2 & 1 & 1 \\
  0 & 1 & -1 \\
  0 & 0 & -1
  \end{pmatrix}
  \begin{pmatrix}
  x_1 \\ x_2 \\ x_3
  \end{pmatrix}
  =
  \begin{pmatrix}
  3 \\ -1 \\ -1
  \end{pmatrix}
  $$

- We can now continue the Gaussian elimination process as usual.

*In general*. Gaussian elimination requires row swaps to avoid breaking down when there is a zero in the "pivot" position.

## Problems with finite precision

Consider using Gaussian elimination to solve the linear system of equations given by

$$
\begin{pmatrix}
\varepsilon & 1 \\
1 & 1
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
 =
\begin{pmatrix}
2 + \varepsilon \\ 3
\end{pmatrix}
$$

where $\varepsilon \neq 1$.

- The true, unique solution is $(x_1, x_2)^T = (1, 2)^T$.

- If $\varepsilon \neq 0$, Gaussian elimination gives

  $$
  \begin{pmatrix}
  \varepsilon & 1 \\
  0 & 1 - \frac{1}{\varepsilon}
  \end{pmatrix}
  \begin{pmatrix}
  x_1 \\ x_2
  \end{pmatrix}
   =
  \begin{pmatrix}
  2 + \varepsilon \\ 3 - \frac{2 + \varepsilon}{\varepsilon}
  \end{pmatrix}
  $$

- Problems occur not only when $\varepsilon = 0$ but also when it is very small, i.e. when $\frac{1}{\varepsilon}$ is very large, this will introduce very significant rounding errors into the computation.

### Removing the problem

Use Gaussian elimination to solve the linear system of equations given by

$$
\begin{pmatrix}
1 & 1 \\
\varepsilon & 1
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
 =
\begin{pmatrix}
3 \\ 2 + \varepsilon
\end{pmatrix}
$$

where $\varepsilon \neq 1$.

- The true solution is still $(x_1, x_2)^T = (1, 2)^T$.

- Gaussian elimination now gives

  $$
  \begin{pmatrix}
  1 & 1 \\
  0 & 1 - \varepsilon
  \end{pmatrix}
  \begin{pmatrix}
  x_1 \\ x_2
  \end{pmatrix}
  =
  \begin{pmatrix}
  3 \\ 2 - 2\varepsilon
  \end{pmatrix}
  $$

- The problems due to small values of $\varepsilon$ have disappeared.

### Notes

- Writing the equations in a different order has removed the previous problem.

- The diagonal entries are now always *relatively* larger.

- The interchange of the order of equations is a simple example of **row pivoting**. This strategy avoids excessive rounding errors in the computations.

## Gaussian elimination with pivoting

- Before eliminating entries in column $j$:

  - find the entry in column $j$, below the diagonal, of maximum magnitude;
  - if that entry is larger in magnitude than the diagonal entry then swap its row with row $j$.

- Then eliminate column $j$ as before.

### Notes

- This algorithm will always work when the matrix $A$ is invertible/non-singular.

- Conversely, if all of the possible pivot values are zero this implies that the matrix is singular and a unique solution does not exist.

- At each elimination step the row multiplies used are guaranteed to be at most one in magnitude...

- ... so any errors in the representation of the system cannot be amplified by the elimination process.

- As always, solving $A \vec{x} = \vec{b}$ requires that the entries in $\vec{b}$ are also swapped in the appropriate way.

- Pivoting can be applied in an equivalent way to LU factorisation.

- The sequence of pivots is independent of the vector $\vec{b}$ and can be recorded and reused.

- The constraint imposed on the row multipliers means that for LU factorisation every entry in $L$ satisfies $| l_{ij} | \le 1$.

In python, the function call
```python
P, L, U = scipy.linalg.lu(A, permute_l=0)
```
factorises $A$ and returns $L$, $U$ and the **pivot matrix** $P$.

### Example

Consider the linear system of equations given by

$$
\begin{pmatrix}
10 & -7 & 0 \\
-3 & 2.1 - \varepsilon & 6 \\
5 & -1 & 5
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
7 \\ 9.9 + \varepsilon \\ 11
\end{pmatrix}
$$

where $0 \le \varepsilon \ll 1$, and solve it using

1. Gaussian elimination without pivoting

2. Gaussian elimination with pivoting.

The exact solution is $\vec{x} = (0, -1, 2)^T$ for any $\varepsilon$ in the given range.

#### 1. Solve the system using Gaussian elimination with no pivoting.

Eliminating the first column gives

$$
\begin{pmatrix}
10 & -7 & 0 \\
0 & -\varepsilon & 6 \\
0 & 2.5 & 5
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
7 \\ 12 + \varepsilon \\ 7.5
\end{pmatrix}
$$

and then the second column gives

$$
\begin{pmatrix}
10 & -7 & 0 \\
0 & -\varepsilon & 6 \\
0 & 0 & 5 + 15/\varepsilon
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
7 \\ 12 + \varepsilon \\ 7.5 + 2.5(12 + \varepsilon)/\varepsilon
\end{pmatrix}
$$

which leads to

$$
x_3 = \frac{3 + \frac{12 + \varepsilon}{\varepsilon}}{2 + \frac{6}{\varepsilon}} \qquad
x_2 = \frac{(12 + \varepsilon) - 6x_3}{-\varepsilon} \qquad
x_1 = \frac{7+ 7x_2}{10}.
$$

There are many divisions by $\varepsilon$, so we will have problems if $\varepsilon$ is small.

#### 2. Solve the system using Gaussian elimination with pivoting.

The first stage is identical (because $a_{11} = 10$ is largest).

$$
\begin{pmatrix}
10 & -7 & 0 \\
0 & -\varepsilon & 6 \\
0 & 2.5 & 5
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
7 \\ 12 + \varepsilon \\ 7.5
\end{pmatrix}
$$

but now $|a_{22}| = \varepsilon$ and $|a_{32}| = 2.5$ so we swap rows 2 and 3 to give

$$
\begin{pmatrix}
10 & -7 & 0 \\
0 & 2.5 & 5 \\
0 & -\varepsilon & 6
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
7 \\ 7.5 \\ 12 + \varepsilon
\end{pmatrix}
$$

Now we may eliminate column 2:

$$
\begin{pmatrix}
10 & -7 & 0 \\
0 & 2.5 & 5 \\
0 & 0 & 6 + 2 \varepsilon
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
 =
\begin{pmatrix}
7 \\ 7.5 \\ 12 + 4 \varepsilon
\end{pmatrix}
$$

which leads to the exact answer:

$$
x_3 = \frac{12 + 4\varepsilon}{6 + 2 \varepsilon} = 2 \qquad
x_2 = \frac{7.5 - 5x_3}{2.5} = -1 \qquad
x_1 = \frac{7 + 7 x_2}{10} = 0.
$$

## Further reading

- Mathematics stackexchange: [what are pivot numbers in LU decomposition? please explain me in an example](https://math.stackexchange.com/questions/485513/what-are-pivot-numbers-in-lu-decomposition-please-explain-me-in-an-example)

- [Gaussian elimination with Partial Pivoting](http://www.math.iitb.ac.in/~neela/partialpivot.pdf) [pdf]
- [Gaussian elimination with partial pivoting example](http://www.math.sjsu.edu/~foster/m143m/partial_pivoting_example1.pdf) [pdf]
- [$A = LU$ and solving systems](http://staff.imsa.edu/~fogel/LinAlg/PDF/14%20Solving%20Factored%20Systems.pdf) [pdf]

- Trefethen, Lloyd N.; Bau, David (1997), Numerical linear algebra, Philadelphia: Society for Industrial and Applied Mathematics, ISBN 978-0-89871-361-9.


## Further reading

- Wikipedia: [LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition)
- Wikipedia: [Matrix decomposition](https://en.wikipedia.org/wiki/Matrix_decomposition) (Other examples of decompositions).
- Nick Higham: [What is an LU factorization?](https://nhigham.com/2021/04/20/what-is-an-lu-factorization/) (a very mathematical treatment with additional references)

Note that these implementation use additional pivoting to achieve better results. We tackle this in the next section.

- LAPACK: [`dgetrf()`](http://www.netlib.org/lapack/explore-html/dd/d9a/group__double_g_ecomputational_ga0019443faea08275ca60a734d0593e60.html). (Implementation of LU factorisation from LAPACK).
- Scipy [`scipy.linalg.lu`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html)


## Further reading

- Wikipedia: [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination)
- Joseph F. Grcar. [How ordinary elimination became Gaussian elimination](https://doi.org/10.1016/j.hm.2010.06.003). Historia Mathematica. Volume 38, Issue 2, May 2011. (More history)
- The method is usually implemented via the LU factorisation method we learn next!

