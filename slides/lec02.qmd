---
echo: false
title: COMP2870 Theoretical Foundations of Computer Science II
subtitle: Introduction to systems of linear equations
author:
  - name: Dr. Thomas Ranner (Tom)
    email: T.Ranner@leeds.ac.uk
    affiliation: School of Computer Science, University of Leeds
code-line-numbers: true
html-math-method: mathjax
include-in-header: ../mathjax.html

execute:
  freeze: auto
  cache: true

format:
  live-revealjs:
    slide-level: 3
    slide-number: true
    preview-links: auto
---
# Introduction to systems of linear equations

::: {.callout-tip}
## Module learning objective

Define and identify what it means for a set of vectors to be a basis, spanning
set or linearly independent.
:::

```{python}
import time

import numpy as np
import scipy as sp
from matplotlib import pyplot as plt

plt.style.use("seaborn-v0_8-colorblind")


```

## Definition of systems of linear equations

Given an $n \times n$ matrix $A$ and an $n$-vector $\vec{b}$, find the
$n$-vector $\vec{x}$ which satisfies:
\begin{equation}
\label{eq:sle}
A \vec{x} = \vec{b}.
\end{equation}

### Expanded format:

We can also write \eqref{eq:sle} as a system of linear equations:
\begin{align*}
\text{Equation 1:} &&
a_{11} x_1 + a_{12} x_2 + a_{13} x_3 + \cdots + a_{1n} x_n & = b_1 \\
\text{Equation 2:} &&
a_{21} x_1 + a_{22} x_2 + a_{23} x_3 + \cdots + a_{2n} x_n & = b_2 \\
\vdots \\
\text{Equation i:} &&
a_{i1} x_1 + a_{i2} x_2 + a_{i3} x_3 + \cdots + a_{in} x_n & = b_i \\
\vdots \\
\text{Equation n:} &&
a_{n1} x_1 + a_{n2} x_2 + a_{n3} x_3 + \cdots + a_{nn} x_n & = b_n.
\end{align*}

### Notes

- The values $a_{ij}$ are known as **coefficients**.

- The **right-hand side** values $b_i$ are known and are given to you as part
    of the problem.

- $x_1, x_2, x_3, \ldots, x_n$ are **not** known and are what you need to find
    to solve the problem.

## Can we do it?

Our first question might be: Is it possible to solve \eqref{eq:sle}?

We know a few simple cases where we can answer this question very quickly:

1. If $A = I_n$, the $n \times n$ *identity matrix*, then we *can* solve this
   problem: $$\vec{x} = \vec{b}.$$

### Can we do it?

2. If $A = O$, the $n \times n$ *zero matrix*, and $\vec{b} \neq \vec{0}$, the
   zero vector, then there are infinitely many solutions to the problem:
   $$
   O \vec{x} = \vec{0} \neq \vec{b} \quad \text{for any vector} \quad \vec{x}.
   $$

### Can we do it?

3. If $A$ is *invertible*, with inverse $A^{-1}$, then we *can* solve this
   problem:
   $$
   \vec{x} = A^{-1} \vec{b}.
   $$
   However, in general, this is a *terrible* idea and we will see algorithms
   that are more efficient than finding the inverse of $A$.

### How about finding the inverse of $A$?

```{python}
# | echo: False
def approach1(A, b):
    """
    Solve the system of linear equations Ax = b by applying the inverse of A
    """
    A_inv = np.linalg.inv(A)
    x = A_inv @ b
    return x


def approach2(A, b):
    """
    Solve the system of linear equations Ax = b through numpy's linear solver
    """
    x = np.linalg.solve(A, b)
    return x


def approach3(A_sparse, b):
    """
    Solve the system of linear equations Ax = b through scipy's sparse linear
    solver
    """
    x = sp.sparse.linalg.spsolve(A_sparse, b)
    return x


```

```{python}


def timeit(method, A, b, repeats=5):
    start_time = time.perf_counter()
    for _ in range(repeats):
        method(A, b)
    end_time = time.perf_counter()

    return (end_time - start_time) / repeats


times1, times2, times3 = [], [], []
sizes = [2**j for j in range(3, 14)]

for size in sizes:
    # create a system of linear equations with unique solution
    A_sparse = sp.sparse.diags(
        [-1 * np.ones(size - 1), 3 * np.ones(size), -1 * np.ones(size - 1)],
        offsets=[-1, 0, 1],
        format="csr",
    )
    A = A_sparse.toarray()
    b = np.ones((size,))

    # time solutions
    time1 = timeit(approach1, A, b)
    time2 = timeit(approach2, A, b)
    time3 = timeit(approach3, A_sparse, b)

    times1.append(time1)
    times2.append(time2)
    times3.append(time3)

# plot results
plt.loglog(sizes, times1, label="$A^{-1} \\vec{b}$")
plt.loglog(sizes, times2, label="simple alg.")
plt.loglog(sizes, times3, label="specialised alg.")

plt.xlabel("size")
plt.ylabel("time (s)")
plt.legend()
plt.grid(True)

plt.show()
```

## Key definitions

### Span of vectors

::: {#def-span}
Given a set of vectors of the same size, $S = \{ \vec{v}_1,
\ldots, \vec{v}_k \}$, we say the *span* of $S$ is the set of all vectors which
are linear combinations of vectors in $S$:
\begin{equation}
\mathrm{span}(S) = \left\{ \sum_{i=1}^k x_i \vec{v}_i : x_i \in \mathbb{R}
\text{ for } i = 1, \ldots, k \right\}.
\end{equation}
:::

### Linear dependence of vectors

::: {#def-linear-indep}
Given a set of vectors of the same size, $S = \{\vec{v}_1, \ldots, \vec{v}_k
\}$, we say that $S$ is *linearly dependent*, if there exist numbers $x_1, x_2,
\ldots x_k$, not all zero, such that
\begin{align*}
\sum_{i=1}^k x_i \vec{v}_i = \vec{0}.
\end{align*}
The set $S$ is *linearly independent* if it is not linearly dependent.
:::

### Basis

::: {#def-basis}
We say that a set of $n$-vectors $S$ is a *basis* of a set of $n$-vectors $V$ if
the span of $S$ is $V$ and $S$ is linearly independent.
:::

::: {#thm-basis-expansion}
Let $S$ be a basis of $V$. Then any vector in $V$ can
be written *uniquely* as a linear combination of entries in $S$.
:::



### Another characterisation: the determinant

::: {#def-determinant}
Let $A$ be a square $n \times n$ matrix.

If $n = 2$,
\begin{equation}
\det A = \det \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}
= a_{11} a_{22} - a_{21} a_{12}.
\end{equation}
:::

### Definition part 2

If $n = 3$,
\begin{align*}
\det A & = \det \begin{pmatrix} a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{pmatrix} \\
& = a_{11} (a_{22} a_{33} - a_{23} a_{32}) - a_{12} (a_{21} a_{33} - a_{23}
a_{31}) \\
& \qquad + a_{13} (a_{21} a_{32} - a_{22} - a_{31}).
\end{align*}

### Definition part 3

For general $n$, the determinant can be found by, for example, Laplace expansions
$$
\det A = \sum_{j=1}^n (-1)^{j+1} a_{1,j} m_{1,j},
$$
where $a_{1,j}$ is the entry of the first row and $j$th column of $A$ and
$m_{1,j}$ is the determinant of the submatrix obtained by removing the first row
and the $j$th column from $A$.

### Key theorem

::: {#thm-det}
Let $A$ be an $n \times n$ matrix and $\vec{b}$ be an $n$-column vector.
The following are equivalent:

1. $A \vec{x} = \vec{b}$ has a unique solution.
2. The columns of $A$ form a basis of $\mathbb{R}^n$.
3. $\det A \neq 0$.
:::

### Determinant in `numpy`

```{pyodide}
import numpy as np

A = np.array([[3.0, 1.0], [0.0, 3.0]])
np.linalg.det(A)
```

You will also see different ways to compute determinants in the lab sessions!

## Special types of matrices

The general matrix $A$ before the examples is known as a **full** matrix: any of
its components $a_{ij}$ might be nonzero.

### Triangular matrix

One common (and important) structure takes the form

$$
A = \begin{pmatrix}
a_{11} & 0 & 0 & \cdots & 0 \\ a_{21} & a_{22} & 0 & \cdots &
 0 \\ a_{31} & a_{32} & a_{33} & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots
 & \vdots \\ a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn} \end{pmatrix}.
 $$

### Triangular matrix

- A is a **lower triangular** matrix. Every entry above the leading diagonal
    is zero:

    $$ a_{ij} = 0 \quad \text{ for } \quad j > i. $$

- The *transpose* of this matrix is an **upper triangular** matrix and can be
treated in a very similar manner.

- Note that the determinant of a triangular matrix is simply the product of
  diagonal coefficients:
  $$
  \det A = a_{11} a_{22} \cdots a_{nn} = \prod_{i=1}^n a_{ii}.
  $$

### Sparse matrices

**Sparse matrices** are prevalent in any application which relies on some form
of *graph* structure (see both the
[temperature](https://comp2870-2526.github.io/linear-algebra-notes/#exm-temperature)
and [traffic
network](https://comp2870-2526.github.io/linear-algebra-notes/#exm-traffic)
examples).

- The $a_{ij}$ typically represents some form of "communication" between
    vertices $i$ and $j$ of the graph, so the element is only nonzero if the
    vertices are connected.

- There is no generic pattern for these entries, though there is usually one
    that is specific to the problem solved.

### Sparse matrices

- Usually, $a_{ii} \neq 0$ - the diagonal is nonzero.

- A "large" portion of the matrix is zero.
  - A full $n \times n$ matrix has $n^2$ nonzero entries.
  - A sparse $n \times n$ has $\alpha n$ nonzero entries, where $\alpha \ll
        n$.
- Many special techniques exist for handling sparse matrices, some of which
can be used automatically within Python ([`scipy.sparse`
documentation](https://docs.scipy.org/doc/scipy/reference/sparse.html))

### What is the significance of these special examples?

- In the next section we will discuss a general numerical algorithm for the
    solution of linear systems of equations.

- This will involve **reducing** the problem to one involving a **triangular
    matrix**, which, as we show below, is relatively easy to solve.

- In subsequent lectures, we will see that, for *sparse* matrix systems,
    alternative solution techniques are available.
